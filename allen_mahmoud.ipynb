{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allennlp for Multilingual Text Classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "random.seed(30)\n",
    "dataset = pd.read_csv('../data/dataset.csv', sep='\\t', header=None).applymap(str)\n",
    "dataset.columns = [\"language\",\"label\",\"text\"]\n",
    "languagesData=[]\n",
    "loc = 0\n",
    "languages = dataset[dataset.columns[0]].unique()\n",
    "for i in languages:\n",
    "    name = languages[loc]+\"Data\" \n",
    "    globals()[name] = pd.DataFrame( dataset[dataset.language == i])\n",
    "    loc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "def preprocessing(text):\n",
    "    text = text.str.lower()                                              #Lower\n",
    "    text = text.apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])     #Remove Secured URL\n",
    "    text = text.apply(lambda x: re.split('http:\\/\\/.*', str(x))[0])      #Remove URL\n",
    "    text = text.str.replace('\\d+', '')                                   #Remove_numbers\n",
    "    text = text.str.replace('[^\\w\\s]','')                                #Remove_punctuations\n",
    "    text = text.str.strip()                                              #remove_blank_space\n",
    "    text = text.replace('\\s+', ' ', regex=True)                          \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mCannot uninstall 'PyYAML'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'allennlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6403eef07f13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBucketIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'allennlp'"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from allennlp.training.trainer import Trainer\n",
    "from typing import Dict\n",
    "import logging\n",
    "import csv\n",
    "from overrides import overrides\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import LabelField, TextField, Field\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.tokenizers import Tokenizer, WordTokenizer\n",
    "from allennlp.data.tokenizers.word_splitter import JustSpacesWordSplitter\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "logger = logging.getLogger(__name__)  \n",
    "# @DatasetReader.register(\"data-reader\")\n",
    "class MultilingualDatasetReader(DatasetReader):\n",
    "    def __init__(self,    \n",
    "        lazy: bool = False,\n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy = lazy)\n",
    "        self._tokenizer = tokenizer or WordTokenizer(JustSpacesWordSplitter())\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path):\n",
    "        logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "        with open(file_path, \"r\") as data_file:\n",
    "            tsv_in = csv.reader(data_file, delimiter=',')\n",
    "            for row in tsv_in:\n",
    "                if len(row) == 2:\n",
    "                    Instance = self.text_to_instance( article=row[1],label=row[0])\n",
    "                    yield Instance\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self,  \n",
    "                 article: str,\n",
    "                 label: str = None) -> Instance:\n",
    "        fields: Dict[str, Field] = {}\n",
    "        tokenized_article = self._tokenizer.tokenize(article)\n",
    "        fields[\"tokens\"] = TextField(tokenized_article, self._token_indexers)\n",
    "        if label is not None:\n",
    "            fields['label'] = LabelField(label)\n",
    "        return Instance(fields)\n",
    "    \n",
    "    \n",
    "class LstmClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        \n",
    "            mask = get_text_field_mask(tokens)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.hidden2tag(encoder_out)\n",
    "\n",
    "        \n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "            output[\"acc\"] = self.accuracy(logits, label)\n",
    "\n",
    "\n",
    "        return output\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        metrics = {'accuracy': self.accuracy.get_metric(reset)}\n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.util import JsonDict\n",
    "from allennlp.data import Instance\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "\n",
    "# @Predictor.register('text_classifier')\n",
    "class TextClassifierPredictor(Predictor):\n",
    "    \"\"\"\n",
    "    Predictor for any model that takes in a sentence and returns\n",
    "    a single class for it.  In particular, it can be used with\n",
    "    the :class:`~allennlp.models.basic_classifier.BasicClassifier` model\n",
    "    \"\"\"\n",
    "    def predict(self, sentence: str) -> JsonDict:\n",
    "        return self.predict_json({\"sentence\": sentence})\n",
    "\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        sentence = json_dict[\"sentence\"]\n",
    "        return self._dataset_reader.text_to_instance(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
